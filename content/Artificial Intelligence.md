---
title: "Artificial Intelligence"
date: "2020-02-10"
slug: "artificial-intelligence"
tags: ["keynotes", "notes"]
---

## What is Artificial Intelligence?
Defining __artificial intelligence__ isn’t just difficult; it’s impossible, not the least because we don’t really understand human intelligence. 

Paradoxically, advances in AI will help more to define what human intelligence isn’t than what artificial intelligence is. But whatever AI is, we’ve clearly made a lot of progress in the past few years, in areas ranging from computer vision to game playing. AI is making the transition from a research topic to the early stages of enterprise adoption. Companies such as Google and Facebook have placed huge bets on AI and are already using it in their products. But Google and Facebook are only the beginning: over the next decade, we’ll see AI steadily creep into one product after another. We’ll be communicating with bots, rather than scripted robo-dialers, and not realizing that they aren’t human. We’ll be relying on cars to plan routes and respond to road hazards. It’s a good bet that in the next decades, some features of AI will be incorporated into every application that we touch and that we won’t be able to do anything without touching an application.Given that our future will inevitably be tied up with AI, it’s imperative that we ask: Where are we now? What is the state of AI? And where are we heading?

Descriptions of AI span several axes: __strength__ (how intelligent is it?), __breadth__ (does it solve a narrowly defined problem, or is it general?), __training__ (how does it learn?), __capabilities__ (what kinds of problems are we asking it to solve?), and __autonomy__ (are AIs assistive technologies, or do they act on their own?). Each of these axes is a spectrum, and each point in this many-dimensional space represents a different way of understanding the goals and capabilities of an AI system.

On the strength axis, it’s very easy to look at the results of the last 20 years and realize that we’ve made some extremely powerful programs. Deep Blue beat Garry Kasparov in chess; Watson beat the best Jeopardy champions of all time; AlphaGo beat Lee Sedol, arguably the world’s best Go player. But all of these successes are limited. Deep Blue, Watson, and AlphaGo were all highly specialized, single-purpose machines that did one thing extremely well. Deep Blue and Watson can’t play Go, and AlphaGo can’t play chess or Jeopardy, even on a basic level. Their intelligence is very narrow, and can’t be generalized. A lot of work has gone into using Watson for applications such as medical diagnosis, but it’s still fundamentally a question-and-answer machine that must be tuned for a specific domain. Deep Blue has a lot of specialized knowledge about chess strategy and an encyclopedic knowledge of openings. AlphaGo was built with a more general architecture, but a lot of hand-crafted knowledge still made its way into the code. I don’t mean to trivialize or undervalue their accomplishments, but it’s important to realize what they haven’t done.

How do we get from narrow, domain-specific intelligence to more general intelligence? By “__general intelligence__,” we don’t necessarily mean human intelligence; but we do want machines that can solve different kinds of problems without being programmed with domain-specific knowledge. We want machines that can make human judgments and decisions. That doesn’t necessarily mean that AI systems will implement concepts like creativity, intuition, or instinct, which may have no digital analogs. A general intelligence would have the ability to follow multiple pursuits and to adapt to unexpected situations. And a general AI would undoubtedly implement concepts like “justice” and “fairness”: we’re already talking about the impact of AI on the legal system.

A __self-driving car__ demonstrates the problems we’re facing. To be self-driving, a car needs to integrate pattern recognition with other capabilities, including reasoning, planning, and memory. It needs to recognize patterns, so it can react to obstacles and street signs; it needs to reason, both to understand driving regulations and to solve problems like avoiding obstacles; it needs to plan a route from its current location to its destination, taking into account traffic and other patterns. It needs to do all of these repeatedly, updating its solutions constantly. However, even though a self-driving car incorporates just about all of AI, it doesn’t have the flexibility we’d expect from a general intelligence system. You wouldn’t expect a self-driving car to have a conversation or lay out your garden. Transfer learning, or taking results from one area and applying them to another, is very difficult. You could probably re-engineer many of the software components, but that only points out what’s missing: our current AIs provide narrow solutions to specific problems; they aren’t general problem solvers. You can add narrow AIs ad infinitum (a car could have a bot that talks about where to go; that makes restaurant recommendations; that plays chess with you so you don’t get bored), but a pile of narrow intelligences will never add up to a general intelligence. General intelligence isn’t about the number of abilities, but about integration between those abilities.

While approaches like neural networks were originally developed to mimic the human brain’s processes, many AI initiatives have given up on the notion of imitating a biological brain. We don’t know how brains work; neural networks are computationally useful, but they’re not imitating human thought. In __Artificial Intelligence: A Modern Approach__, Peter Norvig and Stuart Russell write that “The quest for ‘artificial flight’ succeeded when the Wright brothers and others stopped imitating birds and started … learning about aerodynamics.” Similarly, to make progress, AI need not focus on imitating the brain’s biological processes, and instead try to understand the problems that the brain solves. It’s a safe bet that humans use any number of techniques to learn, regardless of what may be happening on the biological level. The same will probably be true of a general artificial intelligence: it will use pattern matching (like AlphaGo), it will use rule-based systems (like Watson), it will use exhaustive search trees (like Deep Blue). None of these techniques map directly onto human intelligence. What humans appear to do better than any computer is to build models of their world, and act on those models.

__Why the Surge of Interest?__
Why is AI currently such a hot topic, after having being in disrepute for a few decades of “AI winter”? Of course, AI was in the news briefly after Deep Blue, and again after Watson; but these fads didn’t last. It’s tempting to see the current rise of AI as another fad. That would ignore the changes of the past decade.

The rise of AI has depended on tremendous advances in computer hardware. It’s tedious to recite the huge advances in performance and storage technology in the 30+ years since the start of the AI winter (which Wikipedia traces to 1984). But that’s an unavoidable part of the story, particularly if you’ve seen the racks of machines that made up IBM’s Watson. AlphaGo reportedly ran on 1,920 CPUs and 280 GPUs; the machine that beat Lee Sedol may have been even larger, and used custom hardware Google has developed for building neural networks. Even if AI algorithms are too slow to be productive on a typical laptop, it’s easy and relatively inexpensive to allocate some serious computing horsepower on cloud platforms like AWS, GCE, and Azure. And machine learning was enabled, in part, by the ability to store vast amounts of data. In 1985, gigabytes were rare, and weighed hundreds of pounds; now gigabytes are commonplace, inexpensive, and tiny.

We’ve also made significant advances in algorithms. Neural networks aren’t particularly new, but “deep learning” stacks up a series of networks, with feedback so the network automatically trains itself. Deep learning thus tries to solve one of the hardest human problems in machine learning: learning optimal representations and features from data. Processing a lot of data is easy, but feature learning is more of an art than a science. Deep learning automates some of that art.

Not only have we made progress in algorithms, the algorithms are implemented in widely available libraries, such as Caffe, TensorFlow, Theano, Scikit-Learn, MXNet, CNTK, and others. AI isn’t limited to CS researchers in academic settings; increasingly, anyone can take part, as Pete Warden has shown. You don’t need to know how to implement a complex algorithm and make it run reasonably well on your hardware. You just need to know how to install a library and tag training data. Just as the PC revolution itself took place when computers moved out of machine rooms and became accessible to the general public, the same process of democratization is producing a revolution in AI. As people from many backgrounds and environments experiment with AI, we’ll see new kinds of applications. Some will seem like science fiction (though self-driving cars seemed like science fiction only a few years ago); there will certainly be new applications that we can’t even imagine.

## Alan Turing and the Turing Test
Alan Turing is a towering figure in computer science and AI. He is often called the “father of AI.”

In 1936, he wrote a paper called “On Computable Numbers.” In it, he set forth the core concepts of a computer, which became known as the Turing machine. Keep in mind that real computers would not be developed until more than a decade later.

Yet it was his paper, called “Computing Machinery and Intelligence,” that would become historic for AI. He focused on the concept of a machine that was intelligent. But in order to do this, there had to be a way to measure it. What is intelligence—at least for a machine?

This is where he came up with the famous “Turing Test.” It is essentially a game with three players: two that are human and one that is a computer. The evaluator, a human, asks open-ended questions of the other two (one human, one computer) with the goal of determining which one is the human. If the evaluator cannot make a determination, then it is presumed that the computer is intelligent.

## The machines learn
Machine learning can be seen from a technical perspective as a subset of statistics. However, that's not how it might look from the outside. For historical reasons, machine learning has evolved largely independently from statistics, in some cases reinventing the same techniques and giving them a different name, and in other cases inventing whole new ideas without statisticians supposedly involved. 

“Machine learning” has become a catchall term that covers a lot of different areas, ranging from classification to clustering. As such, I can't really give you a crisp definition of what it means. However, there are several commonalities that pretty much all machine learning algorithms seem to work with:

- It's all done using computers, leveraging them to do calculations that would be intractable by hand.
- It takes data as input. If you are simulating a system based on some idealized model, then you aren't doing machine learning.
- The data points are thought of as being samples from some underlying “real-world” probability distribution.
- The data is tabular (or at least you can think of it that way). There is one row per data point and one column per feature. The features are all numerical, binary, or categorical.

The last of these properties is the real kicker. Most machine learning algorithms are designed to handle pretty much any tabular dataset, but they ONLY handle tabular data.

Tabular data lends itself to all kinds of mathematical analysis, since the rows of a table with n rows and d columns can be viewed as locations in d-dimensional space. This is why machine learning is easily the most mathematically sophisticated thing a data scientist is likely to do.

In most machine learning applications, the data points are thought of as being drawn from some underlying distribution, and the goal is to find patterns in the samples that tell us something about that distribution as a whole or that will let us process other samples from it.

Machine learning was partly born out of the initial failures of the artificial intelligence (AI) movement. For a long time, people were very focused on the idea that computers could be made to think, and it was widely expected that thinking machines were only a few years away. 