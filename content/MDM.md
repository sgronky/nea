---
title: "Master Data Management"
date: "2019-04-03"
slug: "master-data-management"
tags: ["data-management", "project"]
---

## Technical Architecture
- Web-based Applications Enable Collaboration
- Open Platform supports entire enterprise
- Minimize the System Administration Barrier
- Scalability and Licensing in Sync for Growth
- Reporting Platform Puts Info to Work
- Built-in Features for Best-practice Change Management
- Support ongoing master data stewardship and governance requirements through workflowbased monitoring and corrective-action techniques
- Define the latency and accessibility of master data*

*: _(At one end of the spectrum are requirements for real-time, synchronous reading and writing of master data in a transactional scenario between systems and services. At the other end of the spectrum are requirements that can be addressed by message-based, workflow-oriented scenarios involving distributed tasks across an organization, and legacy-style batch interfaces for the transfer of master data in bulk file format.)_

## Important Aspects
__MEASURING SUCCESS FROM THE BEGINNING__
_"...success metrics should make it much easier to sell the value of MDM back to management and explain how the project has helped the business"_

Building a __business case__ tied to __business objectives__ that
impact the bottom line is key. Position MDM to justify both
the initial and ongoing funding to support a long-term data
management strategy.

__Master Data Management__ is a discipline essential to obtaining a single, consistent view of an enterprise’s core business entities–customers, products, suppliers, employees, and others. MDM solutions enable enterprise-wide master data synchronization. Some subject areas require input from across the enterprise.

__To measure is to know. If you cannot measure it, you cannot improve it__

## Addressing the ROI Question
There is no one recipe for making the case for Customer MDM. Attempts to try to calculate and project __ROI__ will be a swag at best and probably miss the central point that MDM is really an evolving business practice that is necessary to better manage your data, and not a specific project with a specific expectation and time-based outcome that can be calculated up front. Instead, consider all the business dependencies and decisions made that are associated with this data. If anything, the longer-term value of MDM can only be truly measured in real time as cohesive data management and sound governance decisions are made based on ongoing business needs and strategic plans. MDM practices driven by a governance process should certainly consider ROI where possible in making investment and data quality improvement decisions, but MDM, as a developing internal core competency, should be considered as an investment toward improving fundamental data management practices across the company.

Consider what we have identified as the fundamentals of Customer MDM practices. These are process and quality management investment areas that should be justifiable based on any number of existing business problems and data issues. Company A might suffer from such poor data quality that it can barely function. Company B may face severe strict government oversight. In any event, companies first need to recognize the strongest probable benefits of an MDM initiative and build the business case around that. Next, they should estimate how much they are losing by not realizing all the benefits of a having a timely, accurate, and consistent set of data delivered to the company. This is sometimes referred to as activity-based costing (ABC). Often, the best way to measure the potential benefit of MDM involves determining the amount of money that a company spends with reactive activities in place to compensate for a suboptimal set of processes and tools.

## Master Data Management and Hadoop
[MDM-Hadoop](https://www.safaribooksonline.com/library/view/integrating-hadoop/9781634621540/Chapter-8.xhtml)

## Expanding Master Data Management
[Expanding Master Data Management](https://www.dataversity.net/expanding-master-data-management-with-big-data/#)

MDM can feed Big Data by _providing the data model backbone to bind the Big Data facts_. Viewed from this perspective, Master Data Management is a critical prerequisite for Big Data Governance — particularly when one considers the various facets of governance that are a part of any competitive MDM system. Those include aspects of:

- Lifecycle Management  
- Data Quality (Deduplication)  
- Data Cleansing  
- Metadata Management  
- Reference Data Management

## Informatica
- __Universal Data Access__:  The solution you choose should have the ability to access this data from disparate systems, govern, update, and share the latest trusted record back and forth from any system, application, on premise, or in the cloud. Given the sheer number of systems in today’s financial services company, being locked out of your data is a show stopper.  
- __Built in Data Governance__:  Look for a solution that allows business users to collaborate with data stewards and IT to access, govern, and share comprehensive master (e.g. customer, product, employee, counterparties, securities instrument) data.  In addition, look for a solution that provides flexibility in your governance process through configurable workflows and business process orchestration.  
- __Business Rules based Relationships__: Look for a solution that allows you to create business rules to define the relationships between each domain for greater insight and value.  
- __Built to grow with your business needs__:  You may start with a single domain (e.g. Customers) however look for a solution that can manage multiple domains in a single solution.  
- __Proven success and adoption__:  Experience and success goes without saying. It is important to look for a solution that has adoption in the market. Don’t rely on fancy marketing verbiage and a savvy sales person. Look for evidence of adoption.

## Master Data Management and Data Governance
[MDM&DG](...)

__Master Data Management__ (__MDM__) is the framework of processes and technologies aimed at creating and maintaining an authoritative, reliable, sustainable, accurate, and secure data environment that represents a “single and holistic version of the truth” for master data and its relationships, as well as an accepted benchmark used within an enterprise as well as across enterprises and spanning a diverse set of application systems, lines of business, channels, and user communities.

What categories of data should be a priority for the enterprise projects around the single version of truth and data quality? 

Master Data Management resolves this uncertainty in priorities by clearly stating MDM focus. MDM claims that __some entities__ (master entities) __are more important than others because they are widely distributed across the enterprise__ as well as reside and are maintained in multiple systems and application silos.

Bringing “order” to master data often solves 60–80 percent of the most critical and difficult-to-fix data quality problems. Thus, sound enterprise practices in management and governance of master data directly contribute to the success of the organization. Mismanagement of these practices and lack of master data governance pose the highest risk.

Understanding the reasons for embarking on a Master Data Management initiative does not make it easier to accomplish the goals of MDM. Some significant challenges have to be overcome in order to make Master Data Management a reality. As the term “Master Data Management” implies, one of these challenges is centered on how to make data under management a “golden,” authoritative version known as the “master.”

In order to create domain-specific, complete, accurate, and integrated master data, an organization needs to develop and institutionalize processes that help to discover and resolve inconsistencies, incompleteness, and other data quality issues caused in significant part by the way the established enterprises collect, store, and process data.

MDM is much more than traditional data quality initiatives: Whereas most of the data quality initiatives are concerned with improving data quality within the scope of a particular application area or at a level of the specific line of business, MDM is focused on solving data-quality concerns in an integrated fashion across the entire enterprise.

Moreover, MDM is intrinsically linked with enterprise business processes and Business Process Management (BPM) technologies. Indeed, many business processes are designed assuming that accurate, complete data that can be trusted to execute business transactions and make key business decision is available as and when needed, preferably in the form of a trusted authoritative “system of record” (SOR)

### Drivers
- What business requirements drive the need to have access to better quality and more accurate and complete master data while still relying on existing processes and applications?
- What business activities would gain significant benefits from changing not just applications but also key business processes that deal with customer interactions and customer services at every touch point and across front- and back-office operations?

### Partnership with Vendors
Given the complexity of many MDM projects, it is reasonable to assume that no single vendor can provide an all-in-one, out-of-the-box comprehensive MDM solution that fits the business and technical requirements of every enterprise. This should not be looked upon as a limitation of continuously evolving vendor products but rather as another call for the acceptance of a service-oriented architecture philosophy. Conceptually speaking, each vendor provides a service or a combination of services used by an enterprise within its service-oriented infrastructure. This approach is aligned with a vision of an agile enterprise architecture optimized for an evolutionary change. As we know, reliance on a single product that fits all demands or a vendor that serves all enterprise needs leads to monolithic inflexible solutions that cannot evolve to support the changing business needs and open-architecture standards.

This puts MDM implementers in a position where they have to look for a combination of vendor products and home-grown solutions in order to meet the business demands of the enterprise. Involving multiple vendors is not unusual for any major MDM implementation.

### MDM and Securities Master
Financial services institutions manage complex products known as financial instruments. Financial instruments continuously grow in their complexity and variety (currently about 8.4 million). These products include securities, bonds, stocks, deposits, loans, options, futures, derivatives, swaps, collars, and so on. Derivatives are complex products that derive their value from the underlying assets; new derivatives are created frequently by financial services firms, whenever the firm sees an opportunity to offer a new product with attractive characteristics that can help the firm to differentiate itself from the competition and to establish or strengthen customer relationships in the process.

But the issue with derivatives is only one concern. Generally speaking, accurate, timely, and reliable information about the financial instruments is a core reference source for practically all financial services companies, and it should be treated as a special domain of the Master Data Management environment. There are several reasons for this treatment, including a wide variety of identifiers that require rationalization and may be difficult to reconcile. For example, when we look at the most popular identifiers of securities, we can easily see how numerous and diverse these identifiers are.

The Committee on Uniform Security Identification Procedures (CUSIP)6 manages CUSIP as a unique nine-character (alphanumeric) security identifier used for all North American securities and their issuers for the purpose of the clearing and settlement of trades. The CUSIP system is owned by the American Bankers Association. In addition to CUSIP, financial services companies sometimes use CUSIP’s international equivalent, CINS,7 the International Securities Identification Numbering (ISIN) system,8 and other standard and sometimes internal, organization-specific identifiers for financial instruments.

__Note__ _The International Securities Identification Numbering (ISIN) system is an international standard set up by the International Organization for Standardization (ISO) and used for numbering specific securities, such as stock, bonds, options, and futures. ISIN numbers are administered by a National Numbering Agency (NNA) in each country, and they work just like serial numbers for those securities._

In addition to the identifier problem for existing securities, information on new securities and updates on existing securities is often unavailable in a timely fashion, and it’s not unusual that the information about the securities is incomplete or inaccurate. In the absence of a systematic MDM solution, inconsistencies in securities description and indicative information can be reconciled only through manual processes. As a result, many trading activities can be negatively impacted by the lack of accurate and timely securities reference data. This poor data quality and unavailability delays trades clearing and settlement, and in general can prevent real-time trade matching, Straight-Through Processing, and next day settlement (T+1) activities.

This discussion, although focused on the financial instruments and securities, clearly illustrates that with the variety and complexity of financial instruments, product-centric variants of MDM are growing in importance. In combination with the need to manage positions, issuers, and other entities associated with product-centric MDM, its data models can be fairly complex.

## Multi-Domain Master Data Management
__Master Data Management (MDM)__ is the application of discipline and control over master data to achieve a consistent, trusted, and shared representation of the data. This is often referred to as achieving a “single version of the truth.” In MDM, business and information technology (IT) disciplines such as data governance, data stewardship, data integration, Data Quality Management (DQM), and metadata management are applied to ensure that there is accuracy, consistency, stewardship, and control of the master data in these domains.

MDM is often thought of as a technical discipline, and it is certainly more familiar to those in IT roles than those in business roles. But a closer look at a well-functioning MDM program reveals that while technology is an integral component that provides MDM with capabilities to operate more efficiently, it is actually the underlying organization and cohesiveness of people and processes associated with the business and IT practices that form the foundation by which MDM becomes a core competency and enterprisewide discipline.

### Maturity States
MDM and data governance maturity models tend to reflect either a behavioral or a functional approach for measuring states of maturity:
- __Behavioral__: Expressing maturity from a behavioral perspective using maturity phase descriptions such as Unaware, Undisciplined, Reactive, Disciplined, Effective, Proactive, and Advanced  
- __Functional__: Expressing maturity from a functional perspective using maturity phase descriptions such as Unstructured, Structured, Foundational, Managed, Repeatable, and Optimized

### Performance Measurements
[https://learning.oreilly.com/library/view/multi-domain-master-data/9780128008355/B9780128008355000117.xhtml](https://learning.oreilly.com/library/view/multi-domain-master-data/9780128008355/B9780128008355000117.xhtml)

## Technical features of MDM solutions
- Creation and management of the core data stores
- Management of processes that implement data governance and data quality
- Metadata management
- Extraction, transformation, and loading of data from sources to target
- Backup and recovery
- Customer analytics
- Security and visibility
- Synchronization and persistence of data changes
- Transaction management
- Entity matching and generation of unique identifiers
- Resolution of entities and relationships

We can define several key tenets of the information management aspects of the MDM architecture that have a profound impact on the design, implementation, and use of the MDM platform:

- Decouple information from applications and processes to enable its treatment as a strategic asset.

- Support the notion that the information content (master data) shall be captured once and validated at the source to the extent permissible by the context.

- Support propagation and synchronization of changes made by MDM system to key master attributes so the changes are available to the consuming downstream systems.

- Enable measurement, assessment, and management of data quality in accordance with information quality standards established by the organization and articulated as part of business needs and data governance.

- Ensure data security, integrity, and appropriate enterprise access.

- Support the retention of data at the appropriate level of granularity.

- Provide an effective vehicle for standardizing content and formats of sources, definitions, structures, and usage patterns.

- Enable consistent, metadata-driven definitions for all data under management.

- Preserve data ownership and support well-defined data governance rules and policies administered and enforced by an enterprise data governance group.

## Analytical vs Operational MDM
- __Analytical MDM__ supports business processes and applications that use master data primarily to analyze business performance and provide appropriate reporting and analytical capabilities, often by directly interfacing with business intelligence (BI) tools and packages. Analytical MDM tends to be read-mostly, it usually does not change or create source data in the operational systems, but it does cleanse and enrich data in the MDM Data Hub. From the overall system architecture view, Analytical MDM can be architected as a feed into the data warehouse and can create or enrich an accurate, integrated view of the master data inside the data warehouse. BI tools are typically deployed to access this cleansed, enriched, and integrated data for reporting, perform deep analytics, and provide drill-through capabilities for the required level of detail.

- __Operational MDM__ allows master data to be collected, changed, and used to process business transactions; Operational MDM is designed to maintain the semantic consistency of the master data affected by the transactional activity. Operational MDM provides a mechanism to improve the quality of the data in the operational systems, where the data is usually created. By design, Operational MDM systems ensure that the accurate, single version of the truth is maintained in the MDM Data Hub and propagated to the core systems used by existing and new processes and applications.

## Enterprise Information Architecture's Principles
- __Principle #1__ Information architecture shall be driven by clearly articulated and properly documented business processes.  

- __Principle #2__ No matter what application is used to create a piece of master data content, this content must be validated against the existing master data.  

- __Principle #3__ Any data modifications/corrections to master data can be made only according to the rules and policies established by the business, including the rules of resolving data change conflicts; these changes will be made available to all downstream systems based on agreed upon SLAs.  

- __Principle #4__ Every data item shall have an identified business owner, a custodian (steward), and a single authoritative source that is used by all enterprise stakeholders. We are not making any assumptions about how many systems will be used to capture and update the master data operationally. The authoritative source should obtain all updates in real time and make policy-based decisions about acceptance or rejection of the change for the purpose of enterprise use.  

- __Principle #5__ The quality of data shall be measured in accordance with information quality standards established by the organization.  

- __Principle #6__ Information architecture shall ensure data security, integrity, and appropriate access controls.  

- __Principle #7__ Information architecture shall support the appropriate retention of data at the appropriate level of granularity.  

- __Principle #8__ Sources, definitions, structures, and usage of shared and common information shall be standardized.  

- __Principle #9__ Information architecture shall support the definition, assignment, persistence, and synchronization of unique identifiers for all business objects supported and used by the enterprise.  

- __Principle #10__ Information architecture shall support flexible, accurate, and timely data integration, and promote the creation and maintenance of Master Data Management environments as authoritative systems of record (“single version of the truth”).  

- __Principle #11__ Information architecture shall provide for consistent, metadata-driven definitions for all data under management.  

- __Principle #12__ Information management will include and be based on well-defined data governance rules and policies administered and enforced by appropriately structured and empowered groups, including an Enterprise Data Governance group.  

## Data Stewardship and Ownership
One of the key principles of the enterprise information architecture (Principle #4) states that every data item shall have an identified business owner and a custodian (steward).

As the name implies, __data owners__ are those individuals or groups within the organization that are in the position to obtain, create, and have significant control over the content (and sometimes, access to and the distribution of) the data. Data owners often belong to a business rather than a technology organization. For example, an insurance agent may be the owner of the list of contacts of his or her clients and prospects.

The concept of __data stewardship__ is different from data ownership. Data stewards do not own the data and do not have complete control over its use. Their role is to ensure that adequate, agreed-upon quality metrics are maintained on a continuous basis. In order to be effective, data stewards should work with data architects, database administrators, ETL (Extract, Transform, Load) designers, business intelligence and reporting application architects, and business data owners to define and apply appropriate data usage policies and data quality metrics. These cross-functional teams are responsible for identifying deficiencies in systems, applications, data stores, and processes that create and change data and thus may introduce or create data quality problems. One consequence of having a robust data stewardship program is its ability to help the members of the IT organization to enhance appropriate architecture components to improve data quality, availability, and integrity.

## Business Case
[MDM Business Case](https://learning.oreilly.com/library/view/master-data-management/9780071744584/ch12.html)

## Main Vendors
__IBM__
Three key MDM products: InfoSphere Master Data Management Server (MDM Server), InfoSphere MDM Server for PIM (Product Information Master), and Initiate Master Data Service (MDS).

__Oracle__
Oracle approaches the market with Oracle Master Data Management Suite. The suite includes a number of data-domain-specific MDM Data Hub products:

- Oracle Customer Hub  
- Oracle Site/Location Hub  
- Oracle Supplier Hub  
- Oracle Product Hub  
- Oracle Higher Education Constituent Hub  

Specific for the financial market: __Oracle Financial Consolidation Hub__

__Informatica__
Informatica, a leader in the data integration space, has been moving to the MDM market gradually. With the acquisition of Similarity Systems in January of 2006, Informatica took a significant step toward getting a leadership position in data quality as it relates to the MDM, especially the Customer domain. Similarity Systems expanded the traditional Informatica offering in the ETL and data integration areas by bringing strong data profiling, data standardization, and matching capabilities. The Informatica Data Quality Suite is built on Similarity Systems’ ATHANOR product, and it delivers data standardization, data matching, and data quality monitoring capabilities to its users.

__SAP__
SAP started out as a premier vendor in the Enterprise Resource Planning (ERP) market. It has rapidly evolved into a major player in Master Data Management. SAP offers a well-known and widely deployed suite of business applications. One of them, MySAP Customer Relationship Management, is designed for customer-centric solutions, including MDM capabilities that are optimized for Transaction-style implementations. SAP solutions provide an integrated set of information management capabilities, including catalog and content management as well as product life-cycle management (PLM), with a particular focus on marketing, merchandising, and promotions (these features came from the acquisition of A2i in 2004).

---

## Soluzione Target: Caratteristiche
Di seguito alcuni aspetti della soluzione target:

- __True MDM__: Tutti i dati relativi alle anagrafiche, i prezzi, ecc... dovrebbero essere recuperati da SmartCo. Ogni nuova richiesta di censimento dovrebbe passare dalla soluzione.
- __Data accessibility__: come conseguenza del ruolo di unica sorgente del dato e fonte di verità, il dato dovrà essere ovunque fruibile -> Valutare la possibilità di creare un'interfaccia web semplificata e custom in grado di rendere disponibili i dati di anagrafica memorizzati, eseguire ricerche intelligenti, raccogliere richieste di nuovi censimenti. Verificare se sia possibile utilizzare le API esposte da SmartCo per l'interazione con il BE. In generale si potrebbe valutare di esporre uno strato di API per la gestione dell'interazione con il sistema, con un gateway in grado di controllare le interazioni.
- __Estendibilità e Data Governance__: Per poter garantire la qualità necessaria del dato, la soluzione deve essere il più possibile estendibile in termini di algoritmi di data governance e di calcolo finanziario
- __Publish / Subscribe__: I sistemi downstream dovrebbero essere informati delle modifiche su un certo titolo e/o su nuovi titoli inseriti.
- __MDM Analitico__: La soluzione dovrebbe consentire l'implementazione di algoritmi avanzati di analisi e fornire strumenti per l'aggregazione delle informazioni.
- __Gestione amministrativa__: Per quanto possibile, la gestione amministrativa dovrebbe essere automatizzata. Questo garantirebbe una maggiore scalabilità -> p.e. l'attività di censimento dei nuovi strumenti, l'attività di estrazione dei report
- __Scalabilità__: Affinché il sistema possa essere fruito in futuro da un numero crescente di Legal Entity e possa accomodare un maggior volume di dati, occorre valutare aspetti di scalabilità differenti: tecnologici (per permettere l'allocazione efficiente di volumi di dati elevati mantenendo performance accettabili), economici (per rendere sostenibile per quanto possibile i costi maggiori di licenza derivanti da un numero maggiore di utenti)...
- __Misure__: Il sistema dovrebbe fornire sufficienti evidenze di misure per consentire di valutare l'utilizzo del sistema, il costo sostenuto, le richieste ricevute, eventuali errori (per supportare al meglio la problem determination nel caso di Alert e Job falliti)
- __Strutturazione e organizzazione IT__: il processo di change dovrebbe essere per quanto possibile automatizzato -> versioning, test automation e valutazione della qualità, infrastructure as code, deploy automation. Più in generale, occorrerebbe avere e formare un __service team__ in grado di supportare tutto il ciclo di business change: analisi, sviluppo, test, deploy e manutenzione. Con il supporto di figure terze a contorno (Devops, Ops, Quality, Security), porterebbe enormi benefici in termini di velocità di esecuzione e qualità del risultato, in quanto sarebbe decisamente orientato al cliente (o al mercato, come si dice in letteratura) [https://www.safaribooksonline.com/library/view/the-devops-handbook/9781457191381/DOHB-ch_07.xhtml]
- __Big Data & MDM__: L'MDM potrebbe essere una parte di una soluzione più ampia in cui oltre ai dati certificati, siano disponibili una serie di dati correlati e a contorno (p.e. news, feed, transazioni, ecc...) sui quali poter operare al fine delle analisi. La potenzialità in termini di BI è infatti notevole. Potrebbero essere implementati algoritmi di IA, ecc... -> in questo senso si potrebbe valutare la possibilità di lavorare in cloud su AWS con cluster EMR. Inoltre si potrebbe in questo senso non estendere la gestione dei dati su SmartCo ma operare a livello di database virtuale, ospitando su Data Lake solo la golden copy, estesa con tutta una serie di campi aggiuntivi.
- __Cloud__: In generale il cloud ci darebbe una serie di vantaggi: monitoring migliorato con gestione ottimizzata dei log e delle misure (su cluster ELK), gestione dei big data out of the box su AWS con cluster EMR e S3, possibilità di implementare le API con i gateway già disponibili (interazione via internet anche con le Boutique), audit automatico, supporto alle modalità DevOps, possibilità di lavorare in parallelo, modalità pay as you go, possibilità di interazione migliorata con bloomberg tramite B-PIPE (già disponibile su AWS), possibilità di utilizzo di cache evolute, possibilità di utilizzo delle lambda function, ecc...
- __Ottimizzazione delle informazioni con Info Provider__: La maggior parte delle informazioni derivano da Bloomberg. In questo senso si potrebbe passare ad una modalità publish / subscribe, in cui ci si sottoscrive ad un titolo e si ricevono le informazioni di cambiamento su quel titolo non appena variano, separando i dati sui prezzi con richieste dedicate. Questo con l'aggiunta di un'interfaccia Web per l'accesso ai dati MDM consolidati, ci permetterebbe di ridurre di molto il numero di terminali Bloomberg da utilizzare, perché gli operatori potrebbero accedere all'applicazione Web invece che al terminale Bloomberg.
- __Processi MDM__. I processi e le procedure operative per la gestione dei dati MDM dovrebbero essere codificati in modo adeguato. P.e. ved. _MDM.md#Enterprise Information Architecture's Principles_

## Aree di approfondimento per la proposta della soluzione
__Riduzione Costi__.
- _TCO attuale vs TCO con modello Service Team e IaaS@AWS_. Occorre recuperare tutte le voci di costo attuali e realizzare delle stime sul TCO target.
- _Risparmio costi sulla integrazione Bloomberg tramite Open Api_. Occorre produrre delle evidenze. P.e. lavorando con il prototipo ELK.
- _Risparmio sull'impegno delle persone attraverso l'ottimizzazione dei processi_. 
- _Risparmio sui costi di licenza della soluzione MDM_. Occorre verificare che sia fattibile estendere il modello MDM al di fuori di SmartCo utilizzando una soluzione in cloud.

__Modello__.
Esplorare la possibilità di utilizzare un modello Service Team in grado di gestire completamente il servizio (dallo sviluppo, al change, alla parte di Ops). Bisognerebbe individuare una società in grado di fornire completamente un team adeguato. P.e. Reply o OJ o altra società da individuare. Le figure dovrebbero essere 5: 2xDev, 1xDevOps, 1xOps, 1xQA. La figure vanno chiaramente motivate.

__Ottimizzazione__.
Per dimostrare il valore di una soluzione a Service Team, occorre anche ragionare sul valore completo che si desidera portare al cliente finale. Per esempio si potrebbe coinvolgere di nuovo il team MDM attuale per disegnare delle Value Stream Map per tutti i principali processi attualmente in essere.

__RoadMap__.
Per considerare fattibile una soluzione target, occorre certamente indicare una possibile roadmap a partire dall'attuale soluzione SmartCo. In questo senso bisogna approfondire con SmartCo che possibilità siano ancora presenti sul prodotto attuale o nelle versioni successive che possano essere utilizzate per l'estensione ad una soluzione target.

## Next Steps
x. Riunione con Bloomberg per consolidare la parte tecnica. Chiedere di eventuali limitazioni e della struttura dei costi.
x. Organizzare riunione con NeoXam per approfondire l'estendibilità del prodotto. In particolare occorre verificare con loro se siano in grado di gestire un modello Pub/Sub con Bloomberg e con i sistemi downstream, se sia possibile estendere in cloud i dati, il limite di utilizzo delle API e in generale quali siano le linee guida per l'estendibilità dei processi.
3. Provare a considerare nuove modalità di integrazione con i sistemi downstream. In particolare occorre fare in modo che il perimetro del servizio si fermi prima dei sistemi a valle e questi non siano considerati parte della soluzione (per ottimizzare il modello di gestione non si deve correre il rischio di contagio con gli attuali servizi). Parlare con SimCorp e valutare tutti gli aspetti tecnici delle attuali integrazioni con le country, con Regulus, con GIL, ecc... eventualmente con approfondimenti con i team associati.
4. Costruire delle dashboard su ELK a supporto della dimostrazione del risparmio dei costi Bloomberg. Ingestion dei dati e costruzione dashboard Kibana.
x. Parlare con GSS Germania per la validazione di una possibile soluzione completa in cloud (in parte il punto 6 deve essere anticipato qui): strato microservizi (GO, NGIX oppure LAMBDA -> pro e contro)
x. Riunione con il team di Roma per esplorare le Value Stream Map e comprendere se ci siano possibilità di ulteriori miglioramenti
x. Completare il disegno di una soluzione con tutto lo stack architetturale. Produrre un disegno completo della soluzione in Visio.
x. Raccogliere tutti i costi attuali per creare un TCO completo da confrontare con la soluzione futura.
x. Definire un TCO per la soluzione target
10. Valutare alternative per una società di consulenza in grado di fornire le persone per il Service Team (profili possibili). P.e.: Reply, Oliver Jameas, Imola Informatica, ... Eventualmente sentire Granata sul tema.
x. Verificare come utilizzare SAS all'interno di una soluzione di questo genere.

---

# Note: Strato microservizi
La possibilità di creare dei microservizi tramite i quali esporre delle API verso il sistema MDM è sicuramente da valutare. In questo senso (e anche per valutazioni più generali sulla soluzione) si potrebbe operare sul cloud AWS. Un aspetto interessante da valutare è quello di decidere se appoggiarsi completamente o comunque estensivamente sui servizi AWS, oppure operare per quanto possibile con strumenti in grado di astrarre quale sia il provider cloud che fornisce effettivamente i servizi. Orientativamente, la differenza si sostanzia per lo più nella possibilità in futuro di poter spostare questi servizi on-premise dal cloud o, viceversa, iniziare ad implementarli on-premise per poi spostarli in cloud. 

L'utilizzo di una soluzione completamente basata su AWS abiliterebbe la possibilità di implementare uno strato di microservizi utilizzando delle lambda. Una novità in questo senso è rappresentata dalla possibilità di utilizzare GO come linguaggio, non implementare uno strato di API Gateway, non utilizzare dei container (o eventualmente poggiarsi su docker per l'istanza dei microservizi ma non fare riferimento ad alcun orchestratore come Kubernetes).

## Call con Bloomberg
- Struttura dei costi
- Limiti rispetto a data license e soluzioni batch
- Confronto con le info su Terminali
- Utilizzo diretto delle interfacce REST, senza SDK
- Ambienti di prova

C'è già una B-PIPE (real time market data feed)
Reference data -> data license

## Utilizzo del Cloud nella soluzione attuale
Si potrebbe considerare come primo step verso la creazione di una soluzione completamente custom. Tuttavia, in una prima fase, sarebbe importante poter riuscire a portare a casa rapidamente alcuni vantaggi che una soluzione in cloud AWS potrebbero fornire:

- Infrastructure as Code (che porta al vantaggio di ambienti non produttivi come copie esatte della produzione).
- Continuous Build & Continuous Integration per tutto il codice custom
- Recupero dei debiti tecnici (sviluppi su DB, trigger, stored procedure, codice SSIS, ecc...)
- Scalabilità
- Ridotto costo dell'infrastruttura (da provare)
- Possibilità di integrazione con DataLake (S3 + Glacier)
- Possibilità di monitorare in modo automatizzato i log ed altri parametri
- ...

In questo senso la soluzione potrebbe essere costruita come segue:

1. Macchine EC2 con installato Java (eventualmente su Docker) per Scheduler, Application Server e Tomcat
2. RDS come database su Engine SQLServer
3. Strato Microservizi su Lambda (Go)
4. CloudFront come strato di cache e CDN
5. Monitoraggio su CloudWatch + ELK (con Kibana personalizzata per la verifica dei processi e di alcuni altri dati di Business)
6. Instrumentazione con Prometheus per le metriche fondamentali sull'esecuzione dei processi
7. Golden Copy su S3 (con policy di archiviazione che dopo l'anno potrebbero andare su Glacier)
8. Cluster EMR (a tendere) per l'implementazione di algoritmi complessi e paralleli
9. Redshift come DWH sui dati salvati su S3 (MDM Analitico)

__Alcune informazioni sul DB__:

- __Database attuale__: <2TB
- __Aumento volume attuale__: <1TB / y

Resta da capire come gestire i flussi e lo spostamento dei file.

## Call con NeoXam (28/03/2019)

- Utilizzo API (licenza, costi, limiti, ecc...)? Ce le inviano
- Atre possibilità di storage dei dati consentite e supportate. Data Lake (HDFS)? Sì, MySql, Postgres, Oracle
- Spostamento in cloud? Sì, dalla versione 6
- Esternalizzazione su Data lake della golden copy? Sì, su S3, intero master file
- Supporto di meccanismi di publish / subscribe (verso gli info provider e verso i sistemi downstream)? Sì, ma solo verso sistemi downstream (tramite Kafka che deve essere fornito esternamente)
- Gestione a eventi? Sì, vedi sopra.
- Estendibilità della soluzione verso algoritmi gestiti esternamente in modalità custom (per data-quality, data-cleansing, calcoli complessi distribuiti, ecc...)? Sì, tramite API (da licenziare)
- Stima di aumento dei costi di licenza per aumento degli utenti e/o delle security
- Gestione del versioning, continuous build, continuous deploy e test automation? No. Supportato solo l'infra as code tramite Kubernetes e Docker
- Docker Container per application server e scheduler? Sì, dalla 6
- Autoscaling? Sì, ufficialmente dalla 6, ma anche dalla vesione 5.2 dovrebbe essere possibile

---

## Da recuperare internamente all'IT
- Numero di incident
- Numero di incident che hanno coinvolto GSS
- Numero di processi Batch falliti
- Numero di change requests e numero di deploy in produzione
- Tempo totale di deploy in produzione (calcolato come stima)

---

## Bloomberg Enterprise Access Point
- Quali sono i limiti di invocazione delle api
- Quali sono le funzionalità coperte e quali dati
- Quali sono le possibilità di storage dei dati
- Quali sono le possibilità di integrazione in cloud (AWS)
- Quali sono le differenze fra questa modalità e Data license (Bulk, ecc...) in termini di dati coperti
- Quali sono le policy? E' possibile elaborare questi dati? E' possibile ridistribuirli internamente?
- E' possibile utilizzare una modalità publish / subscribe?
- E' possibile coprire anche altre tipologie di dati (p.e. feed)?
- E' necessario aprire un nuovo contratto o è possibile utilizzare questa modalità all'interno degli attuali accordi di datalicense?
- Qual è la struttura dei costi? Come scala?
- E' disponibile un ambiente di test o in alternativa un trial?

---

## The Lean Movement
[https://learning.oreilly.com/library/view/the-devops-handbook/9781457191381/DOHB-END-BM.xhtml](https://learning.oreilly.com/library/view/the-devops-handbook/9781457191381/DOHB-END-BM.xhtml)
The Lean Movement started in the 1980s as an attempt to codify the Toyota Production System with the popularization of techniques such as Value Stream Mapping, kanban boards, and Total Productive Maintenance.

Two major tenets of Lean were the deeply held belief that lead time (i.e., the time required to convert raw materials into finished goods) was the best predictor of quality, customer satisfaction, and employee happiness; and that one of the best predictors of short lead times was small batch sizes, with the theoretical ideal being “single piece flow” (i.e., “1x1” flow: inventory of 1, batch size of 1).

Lean principles focus on creating value for the customer—thinking systematically, creating constancy of purpose, embracing scientific thinking, creating flow and pull (versus push), assuring quality at the source, leading with humility, and respecting every individual.

---

## Value Stream Mapping
- Le mappe indicate coprono i principali processi attualmente in essere?
- Come possiamo misurare il valore effettivo portato al Business?

__Considerazioni__:
- La maggior parte dei processi operativi sono oggi abbastanza ottimizzati (alto indice). Tuttavia, le attività di change e molte delle attività di manutenzione che spostano l'operatività (p.e. aggiunta nuovi clienti) sono molto lunghe.
- Alcuni dei processi operativi attualmente previsti, potrebbero essere completamente automatizzati o attraverso nuovi algoritmi automatici di validazione o tramite operazioni batch ad opera del sistema
- Molta parte della comunicazione al cliente finale arriva in formato tradizionale (mail). Dovremmo prediligere per quanto possibile una modalità di fruizione self service

__KPI__:
- Completezza del dato
- Disponibilità (T0, T+1, ecc...)
- Completezza del dato e disponibilità (T0 vs T+1). P.e., a partire dal censimento di un nuovo titolo, quanto tempo deve essere atteso prima che il dato sia completo?

---

## Presentazione finale
### Materiale
1. Presentazione originale del progetto diamond
2. TCO (file Excel)
3. Raccomandazione del progetto Bloomberg
4. Business Case [https://learning.oreilly.com/library/view/master-data-management/9780071744584/ch12.html](https://learning.oreilly.com/library/view/master-data-management/9780071744584/ch12.html)
5. Materiale Profisee
6. Matrice di valutazione Profisee 
7. Questo file
8. Vecchia presentazione
9. VSM
10. Architettura
11. Multi-Domain Master Data Management [https://learning.oreilly.com/library/view/multi-domain-master-data/9780128008355/B9780128008355000014.xhtml](https://learning.oreilly.com/library/view/multi-domain-master-data/9780128008355/B9780128008355000014.xhtml)
12. SmartCo Maintenance Elements.xlsx (SmartCo)
13. Report Tasks.xlsx (SmartCo)

### Story telling sintetico
- __Obbiettivi__. E' emersa la volontà di analizzare il business case relativo alla costruzione di un vero MDM. Per una serie di motivazioni interne ma anche e soprattutto per poter abilitare un nuovo business model e generare revenue e savings, il caso è di sicuro interesse. [1,2,8]
- __Requisiti__. A quello principale si aggiungono poi una serie di ulteriori obbiettivi a contorno: unico framework per l'accesso ai dati degli infoprovider, gestione centralizzata della qualità dei dati con maggiore e immediata disponibilità, ampia esposizione (p.e. a supporto delle boutiques), reporting automatico. [1,2,8]
- __Ulteriori requisti__. A questi requisiti generali si aggiungo poi una serie di desiderata volti ad abilitare nuovi scenari e risolvere alcune attuali inefficienze: anagrafica non centralizzata, interfaccia utente non user-friendly, processo di aggiunta nuovi clienti laborioso, creazione di reporting manuale, ecc... [1,2,3]
- __MDM__. Un MDM è tuttavia molto più di un progetto: è un vero e proprio paradigma, che porta quindi con sè una serie di processi (nuovi o da modificare) con alto impatto organizzativo. Qual è il valore (economico) dei dati di cui parliamo? Qual è l'impatto sul resto dell'azienda di una gestione centralizzata di questi dati? [5,7,8]
- Nel caso in esame, applicare un approccio MDM al dominio delle Securities ha certamente un valore strategico per una serie di fattori: Valore di Business, Volume dei dati, Volatilità, Riusabilità, Complessità. [11]
- __Vendor esterni__. Una piattaforma non è in generale in grado da sola di soddisfare le esigenze di una grande azienda in ambito MDM, qualunque essa sia. SmartCo in più presenta di base una serie di limitazioni (di costo, di flessibilità, di performance, di scalabilità). [5,6,7]
-__SmartCo__. La roadmap di prodotto, l'elevato costo delle funzionalità aggiuntive necessarie a supportare i requisiti, la qualità dei servizi professionali finora offerti ([2]) e lo scarso valore aggiunto finora apportato al team non permettono di considerare la piattaforma un investimento da portare avanti a lungo nel tempo.
-__Altri Prodotti__. E' del resto del tutto illogico considerare di investire in altre soluzioni. Per l'elevato rischio associato ad un elevato costo del quale sarebbe complesso a dir poco comprendere il ROI
- __Trasformazione digitale__. D'altra parte, siamo nel mezzo di una nuova trasformazione digitale ed è difficile proporre un rinnovamento che non vada nella direzione di poter sfruttare queste possibilità: cloud, big data, intelligenza artitificiale, ecc...
- Per questo, la flessibilità, la potenza, la scalabilità, l'efficienza che possano essere ottenuti da una soluzione custom sono inarrivabili. [7,8]
- __Architettura Target__. L'architettura target che oggi possiamo immaginare è quindi custom, in cloud e gestita tramite un modello di servizio basato su un service team dedicato in grado di operare su tutta la catena di sviluppo (sviluppo, test, change, operations, manutenzione). [7,8,10]
- __Il cloud__. I vantaggi del cloud sono enormi: scalabilità, flessibilità, sicurezza, performance, misurabilità, analytics, calcolo computazionale distribuito, estendibilità, ecc... [7,8]
- __Un service team__. Un service team dedicato porterebbe il beneficio di demandare all'IT tutta l'operatività di base e i change. In questo modo, anzitutto, i tempi e la qualità potrebbero essere governati meglio e in secondo luogo, si potrebbe automatizzare molta parte dell'operatività oggi manuale. Infine, verrebbe definita un'interfaccia chiara fra l'IT e il team MDM con il vantaggio risultante di avere tutta la documentazione oggi non prodotta. [7,8]
- __Una soluzione custom__. La soluzione custom prevede un datalake contenente i dati e la golden source, un cluster distribuito per i calcoli, una piattaforma web per la gestione e per l'esposizione dei dati. Inoltre una serie di microservizi per garantire l'operatività con i sistemi downstream e ulteriori modalità di integrazione (file, messaggi, ecc...). [7,8]
- __Sostenibilità della soluzione__. 
- __Estendibilità__. Le estendibilità di questa soluzione sarebbero molte: implementazione di algoritmi finanziari, intelligenza artificiale, analytics, misure predittive, allarmi e incident automatici, ecc... Inoltre, la gestione centralizzata dell'accesso agli infoprovider con un modello dati ben definito e documentato, permetterebbero a tendere di gestire l'accesso mediato alle informazioni fornite dagli infoprovider (in particolare bloomberg) in sostituzione di parte degli attuali terminali. [7,8]
- __TCO__. In termini complessivi, rispetto all'attuale TCO, i costi sarebbero inferiori. [8]
- __Macro-Piano__. Il tutto è da intendersi come un programma e come tale prevede dei tempi lunghi (2 anni), ma scanditi da fasi intermedie (e vantaggi relativi) ben determinabili: primi 6 mesi per portare in cloud la soluzione as-is, successivi sei mesi per creare una prima versione della piattaforma web, successivi 6 mesi per eseguire il decommissioning di smartco. [8,10]
- __Appendice__. Architettura, VSM, TCO, Statistiche su SmartCo [2,9,10,12,13]

### Key Benefits
- Performance & Scalability
- DevOps Model (automatic changes, infrastructure as code, green/blue deploys, etc...)
- Open Platform & Integration (accesses, tracings, auditability, metamodel exposure, etc...)
- Automatic Processing
- Fully Monitored (customizable dashboards)
- Reporting & Analytics (realtime data available for costs and charge back)
- Realtime functionalities
- Decoupling of Sources (possibility of changing info providers with virtually no impacts on downstream systems)
- Re-Modelling (Chance of rethinking the entire model and metadata with no constraints)
- Reduced lead time
- Fully extensible (data quality rules, financial algorithms, etc...)
- Virtually Infinite Storage (historical data easily available)
- Push Notifications (event based)
- Automatic On-Boarding of New Clients

---

## Business Case Challenge
### Alcune considerazioni sulla scelta di una soluzione custom
E' proprio un errore affidarsi ad una piattaforma di mercato. A parte infatti la difficoltà nel trovare una piattaforma in grado di soddisfare appieno le nostre esigenze, le varie necessità di integrazione, le customizzazioni già in essere, i processi in piedi, ecc..., e a parte l'entità di investimento necessario ad acquisire una nuova piattaforma di mercato, integrarla, acquisirne le conoscenze necessarie, ecc... è un errore farlo. Perché finiremmo col mettere tra noi e i dati un filtro che non ci permetterebbe di estrarne appieno il valore. In questo momento particolare abbiamo bisogno di aumentare al massimo il nostro grado di confidenza sul dato, nella sua natura più intima, nel suo grado massimo di purezza. Per poter costruire e abilitare ulteriori possibilità e servizi in futuro. 

### Considerazioni generali sul Business Model
La piattaforma MDM non dovrebbe essere considerata solo dal punto di vista tecnologico, ma come una vera e propria piattaforma di business, una soluzione di business in grado di servire clienti e supportare altre piattaforme. In questo senso, il team dovrebbe operare come un'entità indipendente in grado di supportare insieme business, tecnologia, governance, processi e persone con il potere di operare rapidamente. In questo senso, la responsabilità della piattaforma va vista come una responsabilità end-to-end sull'intero business model sotteso, la responsabilità di fornire la soluzione ed erogare il servizio corrispondente. Questo ci permetterebbe di attuare nuove iniziative di business (innovative) e svilupparle ed eseguirle come se si avesse a disposizione tutto l'IT necessario. In definitiva, dovremmo pensare alla piattaforma come a "MDM-as-a-service".

### Temi da trattare nel Business Case Challenge
x Business Model (-> BMC)
x Economics (-> Excel)
x Impatti sui team e sugli attuali processi (-> VSM)
x Roadmap implementativa (sostenibilità della pianificazione) (-> Slide)
x Analisi e mitigazione dei principali rischi (-> Slide)

### Approfondimento economics e presentazione a Clair
- Predisposizione della presentazione in relazione al progetto DMO
- Sostenibilità di uno stream unico MDM (e relativo team)
- Revisione (e presentazione) degli economics

---

## RFI
In recent years, the availability of business-related data has been increasing steadily and strongly, as well as its importance for key processes in the Company (e.g. Investments decisions and performance monitoring, Accounting, Risk Management, Regulatory capital calculation and monitoring, and so on). On the other hand, a substantial part of high-quality data must be sourced from external providers at a cost that, on aggregate, can be substantial.
In this context, Generali Investments Holding (“GIH”) intends to restructure the Data Management area in order to implement a standardized, centralized, industrialized Master Data Management framework for the provision of Data-related services to Group Clients, while monetizing the huge amount of data under management in order to extract the most value out of it. 

The aims of the target Data Management framework are the following:
- Create a unique “Golden Source” for Reference Data serving current and potentially new clients, under dedicated Service Agreements
- Apply coherently a set of Data Quality and Governance rules / processes to the information managed
- Centralize procurement of data from external providers in a comprehensive organizational governance framework and define a comprehensive contractual framework for Info Providers’ data spending 
- Consolidate the IT framework to ensure sustainability in the medium term and optimization of the interaction with other actors (users and systems)

Currently, GIH is employing a third-party solution (Neoxam Data Hub) which covers only a subset of the reference data.

Given the context described above and the stated objectives, the following information are requested:

- How would you reccomend to proceed further with the acquisition of the necessary technology in order to implement the MDM? Would you consider the integration of a new platform from the market or a custom implementation?
- What kind of architectural pattern / MDM implementation styles would you reccomend to apply in the context described and for the data under management?
- How could your solution garantee both operational and analytics data management functionalities? And how can it accomodate both traditional data analysis and newer analytics techniques?
- How do you ensure a proper and effective integration with the systems  downstream? And how do you reccomend to manage data duplication and / or correlation? How do you garantee correct synchronization of intra-day and end-of-day data? How do you handle the updates?
- Which kind of metrics do you envisage for the customer side to measure the level of quality provided?
- How do you garantee an optimal level of scalability for the solution, both from a business and technical perspective? How can we accomplish the right grade of flexibility that such an MDM-as-a-service must provide to its customers?
- What kind of advantages a cloud-based solution can provide? What kind of risks do you foresee in the cloud adoption?
- How can the platform reach a proper grade of openess?
- How do you reccomend to structure and organize the main workflows around the master data (i.e. stewardship, data management, data governance, data integration and metadata management)?

We kindly ask to provide a response to this RFI by xxx. Responses should be submitted complete and in writing. We also kindly ask that all the requests for information above be answered as concisely as possible while providing all information necessary to understand the outsourcing process proposed.

## RFI Answer: CAPGEMINI
Capgemini has a dedicated team that works on Data Management solutions that makes use of a dedicated proprietary framework __One.Information__. The suggested solution is technology agnostic but they suggest the adoption of Informatica. 

## RFI Answer: DELOITTE
Deloitte suggests to considers the right approach (between make or buy) that best suits our needs, suggesting a progressive adoption of market modules and customizing just the ones that are critical. The cloud is reccomended.

## RFI Answer: PWC
PWC suggests a mixed approach, a market platform for the massive acquisition of data and the related management for the creation of the "golden record" combined with customized implementations for the enrichment of the specific data of the Group. As for the adoption of a market platform, SaaS solutions are often offered. In generale the  market trend is to use standard package instead of Custom built. Among the most known vendors in this area
•   GoldenSource
•   Markit
Simcorp recently acquired AIM to cover the all value chain. This will slightly increase the level of integration between Simcorp and AIM but will likely decrease the adaptability of the system to other platform
